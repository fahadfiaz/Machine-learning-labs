{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3116)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities functions for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_prediction(X_data, beta):\n",
    "    beta = np.dot(X_data, beta)\n",
    "    y_predict = _sigmoid(beta)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss_function_new(X_data, Y_data, beta): \n",
    "    y_hat = y_prediction(X_data, beta)\n",
    "    \n",
    "    l = (Y_data * np.log(y_hat)) + ((1-Y_data)*np.log(1-y_hat))\n",
    "    l = sum(l) \n",
    "    l= -1*l\n",
    "    return np.array(l)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_logreg_GA(X_data, Y_data, X_test, Y_test, lr, lamda, nBatches, itrs):\n",
    "    betas = np.zeros((X_data.shape[1], 1)) \n",
    "    listTestAccr = [] \n",
    "    listTestloss = [] \n",
    "    for k in range(itrs): \n",
    "        oldLoss = logloss_function_new(X_data, Y_data, betas)\n",
    "        \n",
    "        for s in range(math.ceil(len(X_data)/nBatches)): \n",
    "            y_hat = y_prediction(X_data[s*nBatches:(s+1)*nBatches],betas)\n",
    "            \n",
    "            betas = (1 - 2*lr*lamda)*betas - lr * (-2 * np.dot(X_data[s*nBatches:(s+1)*nBatches].T,\n",
    "                                                               Y_data.T[s*nBatches:(s+1)*nBatches]-y_hat))\n",
    "        \n",
    "        y_hat_test = y_prediction(X_test,betas)\n",
    "        predicted_labels = [1 if x >= .5 else 0 for x in y_hat_test]\n",
    "        no_true = np.count_nonzero(predicted_labels == Y_test)\n",
    "        no_false = Y_test.shape[1] - no_true\n",
    "        test_accuracy = no_true/(no_true + no_false)\n",
    "  \n",
    "        newLoss = logloss_function_new(X_data, Y_data, betas) \n",
    "    \n",
    "        listTestAccr.append(test_accuracy)        \n",
    "        listTestloss.append(logloss_function_new(X_test, Y_test, betas))\n",
    "        \n",
    "\n",
    "        if np.abs(newLoss - oldLoss) < 0.00000001: \n",
    "            break\n",
    "        \n",
    "    return listTestloss, listTestAccr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split dataset into train,test,validation set as mentioned in exercise along with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df,targetcol):\n",
    "    X = df.drop(targetcol,axis=1)\n",
    "    Y = df[targetcol]  \n",
    "    X_copy = X.copy()\n",
    "    \n",
    "    Xtrain = X_copy.sample(frac=0.70, random_state=0)\n",
    "    \n",
    "    Xtest = X_copy.drop(Xtrain.index)\n",
    "    Xtest_copy = Xtest.copy()\n",
    "    Xtest = Xtest_copy.sample(frac=0.50, random_state=0)\n",
    "    \n",
    "    Xvalidate = Xtest_copy.drop(Xtest.index)\n",
    "    \n",
    "    \n",
    "    norm_Xtrain = np.linalg.norm(Xtrain, axis = 1, keepdims = True)\n",
    "    Xtrain = Xtrain / norm_Xtrain\n",
    "    norm_Xtest = np.linalg.norm(Xtest, axis = 1, keepdims = True)\n",
    "    Xtest = Xtest / norm_Xtest\n",
    "    norm_Xval = np.linalg.norm(Xvalidate, axis = 1, keepdims = True)\n",
    "    Xval = Xvalidate / norm_Xval\n",
    "    \n",
    "    \n",
    "    Y_copy = Y.copy()\n",
    "    \n",
    "    Ytrain = Y_copy.sample(frac=0.70, random_state=0)\n",
    "    \n",
    "    Ytest = Y_copy.drop(Ytrain.index)\n",
    "    Ytest_copy = Ytest.copy()\n",
    "    Ytest = Ytest_copy.sample(frac=0.50, random_state=0)\n",
    "    \n",
    "    Yvalidate = Ytest_copy.drop(Ytest.index)\n",
    "    \n",
    "    Ytrain = np.matrix(Ytrain)\n",
    "    Ytest = np.matrix(Ytest)\n",
    "    Yval = np.matrix(Yvalidate)\n",
    "    \n",
    "    return Xtrain, Ytrain, Xval, Yval, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('bank.csv', delimiter= ';', header=0)\n",
    "\n",
    "data1 = data1.drop('duration', axis = 1) # dropping coloumn of duration because in exercise 1 backward selection, removing this coloumn gave us minimum aic metric.\n",
    "\n",
    "data1['y'] = data1['y'].map({'yes': 1, 'no': 0}) #changing label to numeric \n",
    "\n",
    "data1 = pd.get_dummies(data1)\n",
    "\n",
    "data1.insert(loc=0,column='bias',value=np.ones(len(data1))) #inserting bias to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = split(data1,\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3165, 51)\n",
      "(678, 51)\n",
      "(678, 51)\n",
      "(1, 3165)\n",
      "(1, 678)\n",
      "(1, 678)\n"
     ]
    }
   ],
   "source": [
    "print (Xtrain.shape)\n",
    "print (Xtest.shape)\n",
    "print (Xval.shape)\n",
    "\n",
    "print (Ytrain.shape)\n",
    "print (Ytest.shape)\n",
    "print (Yval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get random hyperparameter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_hyperparameter_configuration(n, L):\n",
    "    \n",
    "    lambdA = np.arange(1e-5 ,1e-2, (1e-2 - 1e-5)/n)\n",
    "    batch_size = np.random.randint(10, L, size=n, dtype=int)\n",
    "    alpha = np.arange(1e-5,1e-2, (1e-2 - 1e-5)/n)\n",
    "    hyperparameter_configuration = [alpha, lambdA, batch_size]\n",
    "    hyperparameter_configuration = np.vstack(hyperparameter_configuration).T\n",
    "    return hyperparameter_configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate validation loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loss(Xtrain, Ytrain, Xval, Yval, hyperparameter, r):\n",
    "    l_test = []\n",
    "    for h in range(np.shape(hyperparameter)[0]):\n",
    "        loss_t, accu_t = learn_logreg_GA(Xtrain, Ytrain, Xval, Yval, hyperparameter[h,0], hyperparameter[h,1], int(hyperparameter[h,2]), r)\n",
    "        l_test.append(loss_t)\n",
    "    return l_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that uses Hyperband Algorithm for tuning the hyperparameters of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperband(Xtrain, Ytrain, Xval, Yval, max_iter, eta):\n",
    "    \n",
    "    smax = math.floor(np.abs(np.log(max_iter)/np.log(eta))) # number of unique executions of Successive Halving (minus one)\n",
    "    \n",
    "    B = max_iter  * (smax+1) # total number of iterations (without reuse) per execution of Succesive Halving (n,r)\n",
    "    \n",
    "    # Begin Finite Horizon Hyperband outlerloop\n",
    "    for s in range(smax,0,-1):\n",
    "        n = math.ceil((B*eta*s)/(max_iter*(s+1)))  # initial number of configurations\n",
    "        r = int(max_iter/eta**s)  # initial number of iterations to run configurations for\n",
    "        print(\"s: \", s)\n",
    "        T = get_random_hyperparameter_configuration(n,100) # Begin Finite Horizon Successive Halving with (n,r)     \n",
    "        for i in range(s):\n",
    "            # Run each of the n_i configs for r_i iterations and keep best n_i/eta\n",
    "            n_i = math.floor(n/eta**i)\n",
    "            r_i = r*eta**i\n",
    "            L= val_loss(Xtrain, Ytrain, Xval, Yval, T, r_i)\n",
    "            t_k = math.ceil(n_i/eta)\n",
    "            combined_s = np.append(T, L, axis=1)\n",
    "            sorted_s = combined_s[np.argsort(combined_s[:,-1])]\n",
    "            T  = sorted_s[0:t_k, 0:3]\n",
    "        # End Finite Horizon Successive Halving with (n,r)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up HyperBand Algorithm hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 81  # maximum iterations/epochs per configuration\n",
    "eta = 3 # defines downsampling rate (default=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running HyperBand Algorithm to get optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:  4\n",
      "s:  3\n",
      "s:  2\n",
      "s:  1\n"
     ]
    }
   ],
   "source": [
    "s = hyperband(Xtrain, Ytrain, Xval, Yval,max_iter,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25875e-03, 1.25875e-03, 9.60000e+01])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using optimize paramters to run logistic regression and get accuracy and loss on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_t, accu_t = learn_logreg_GA(Xtrain, Ytrain, Xtest, Ytest, 0.00125875,0.00125875, 96 , 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306.1218031669333"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_t[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303834808259587"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu_t[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
