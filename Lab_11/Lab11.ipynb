{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hiIarSgcoIL",
    "outputId": "90e11a68-c6d1-49fe-80f4-d81c6e7845b8"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize\n",
    "import heapq\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk  \n",
    "\n",
    "### Run these two lines if you have not installed the following nltk packages ###\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - 0) Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load and merge dataset with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GvuqTVRbcxnw"
   },
   "outputs": [],
   "source": [
    "def twenty_newsgroup_to_df():\n",
    "    \n",
    "    categories = ['sci.med', 'comp.graphics']\n",
    "    newsgroups_train = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "    df = pd.DataFrame([newsgroups_train.data, newsgroups_train.target.tolist()]).T\n",
    "    df.columns = ['text', 'target']\n",
    "\n",
    "    targets = pd.DataFrame( newsgroups_train.target_names)\n",
    "    targets.columns=['title']\n",
    "\n",
    "    out = pd.merge(df, targets, left_on='target', right_index=True)\n",
    "    df = shuffle(out)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to do preprocessing of textual data to remove punctuation, stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "q346d4v0hmHO"
   },
   "outputs": [],
   "source": [
    "def do_pre_processing(sentence):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english') + list(string.punctuation))  \n",
    "\n",
    "    word_tokens = word_tokenize(sentence)  \n",
    "\n",
    "    filtered_sentence = [token.lower() for token in word_tokens if token.lower() not in stop_words and  len(token)>2]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make vocabulary of unique words from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OOAWhuJC__Gi"
   },
   "outputs": [],
   "source": [
    "def make_vocab_with_features(df,features_to_make):\n",
    "    corporus=[]\n",
    "\n",
    "    ### doing preprocessing on dataset\n",
    "    for index, row in df.iterrows():\n",
    "        corporus.append(do_pre_processing(row['text']))\n",
    "\n",
    "    \n",
    "    wordfreq = {}\n",
    "    for sentence in corporus:\n",
    "        for token in sentence:\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1\n",
    "    most_freq = heapq.nlargest(features_to_make, wordfreq, key=wordfreq.get)\n",
    "    return corporus, most_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert dataset to bag of word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WBv3T4bXAea3"
   },
   "outputs": [],
   "source": [
    "def convert_to_bag_of_words(corporus,most_freq):\n",
    "    sentence_vectors = []\n",
    "    for sentence_tokens in corporus:\n",
    "        sent_vec = []\n",
    "        for token in most_freq:\n",
    "            if token in sentence_tokens:\n",
    "                sent_vec.append(1)\n",
    "            else:\n",
    "                sent_vec.append(0)\n",
    "        sentence_vectors.append(sent_vec)\n",
    "\n",
    "    sentence_vectors = np.asarray(sentence_vectors)\n",
    "    return sentence_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert dataset to TF-IDF word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tOq3aVh7rSN8"
   },
   "outputs": [],
   "source": [
    "def convert_to_tf_idf(corpus,most_freq):\n",
    "    word_idf_values = {}\n",
    "    for token in most_freq:\n",
    "        doc_containing_word = 0\n",
    "        for document in corpus:\n",
    "            if token in document:\n",
    "                doc_containing_word += 1\n",
    "        word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
    "\n",
    "    word_tf_values = {}\n",
    "    for token in most_freq:\n",
    "        sent_tf_vector = []\n",
    "        for document in corpus:\n",
    "            doc_freq = 0\n",
    "            for word in document:\n",
    "                if token == word:\n",
    "                    doc_freq += 1\n",
    "\n",
    "            if len(document)==0:\n",
    "                word_tf = doc_freq\n",
    "            else:\n",
    "                word_tf = doc_freq/(len(document))\n",
    "\n",
    "            sent_tf_vector.append(word_tf)\n",
    "        word_tf_values[token] = sent_tf_vector\n",
    "\n",
    "    tfidf_values = []\n",
    "    for token in word_tf_values.keys():\n",
    "        tfidf_sentences = []\n",
    "        for tf_sentence in word_tf_values[token]:\n",
    "            tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "            tfidf_sentences.append(tf_idf_score)\n",
    "        tfidf_values.append(tfidf_sentences)\n",
    "\n",
    "    tf_idf_model = np.asarray(tfidf_values)\n",
    "    tf_idf_model = np.transpose(tf_idf_model)\n",
    "    return tf_idf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GV71SP0ydYdP"
   },
   "outputs": [],
   "source": [
    "def split(X,Y): \n",
    "    \n",
    "    Xtrain = X[:1400]\n",
    "    Ytrain = Y[:1400]\n",
    "\n",
    "    Xval = X[1400:1700]\n",
    "    Yval = Y[1400:1700]\n",
    "\n",
    "    Xtest = X[1700:]\n",
    "    Ytest = Y[1700:]\n",
    "\n",
    "    \n",
    "    return Xtrain, Ytrain, Xval, Yval, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function which implements naive bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function uses naive baise algorithm to learn the probabilities of each unique words in both target classes and\n",
    "### do prediction respectively\n",
    "\n",
    "def main(Xtrain, Ytrain, Xval, Yval, Xtest, Ytest):\n",
    "    \n",
    "    ### merge feature array and target vector\n",
    "    X_df = pd.DataFrame(Xtrain)\n",
    "    y_df = pd.DataFrame(Ytrain)\n",
    "    full_dataset_train = X_df.copy()\n",
    "    full_dataset_train['target'] = y_df\n",
    "    \n",
    "    ### seperate the dataset into respective classes\n",
    "    med = full_dataset_train[full_dataset_train['target'] == 1]\n",
    "    graphic = full_dataset_train[full_dataset_train['target'] == 0]\n",
    "\n",
    "    ### find probability of each target class\n",
    "    med_prob = med.shape[0]/X_df.shape[0]\n",
    "    graphic_prob = graphic.shape[0]/X_df.shape[0]\n",
    "    \n",
    "    \n",
    "    ### Calculate total words in each target class features\n",
    "    total_words_med=0\n",
    "    total_words_grap=0\n",
    "    for i in range(X_df.shape[1]):\n",
    "        counts_med = med[i].sum()\n",
    "        total_words_med+= counts_med\n",
    "        counts_graph = graphic[i].sum()\n",
    "        total_words_grap+= counts_graph\n",
    "\n",
    "    ### Calculate probabilities of each word in each target class features\n",
    "    each_word_prob_med = []\n",
    "    each_word_prob_graphics = []\n",
    "\n",
    "    for i in range(X_df.shape[1]):\n",
    "        col_count_med = med[i].sum()\n",
    "        prob_for_col_med = (col_count_med + 1)/(total_words_med + X_df.shape[1])\n",
    "        each_word_prob_med.append(prob_for_col_med)\n",
    "\n",
    "        col_count_graph = graphic[i].sum()\n",
    "        prob_for_col_graph = (col_count_graph + 1)/(total_words_grap + X_df.shape[1])\n",
    "        each_word_prob_graphics.append(prob_for_col_graph)\n",
    "    \n",
    "    ### Calculate the probabilty of each new row on respective set using the naive bayes algorithm. \n",
    "    ### For each target class we run the naive bayes algorithm and get final probaility of that row belonging to that respective\n",
    "    ### class. Then we assign the row that class which has higest probabilty. \n",
    "    \n",
    "    ### Do prediction on train sets using naive bayes algorithm\n",
    "    correct = 0\n",
    "    total= 0\n",
    "    for index, row in full_dataset_train.iterrows():\n",
    "\n",
    "        rs = np.array(row)\n",
    "        indexs = list(np.where(rs>=1)[0])\n",
    "\n",
    "        all_sentence_prob_med = sum([each_word_prob_med[i] for i in indexs if i!=X_df.shape[1]]) * med_prob\n",
    "        all_sentence_prob_graph = sum([each_word_prob_graphics[i] for i in indexs if i!=X_df.shape[1]]) * graphic_prob\n",
    "        if all_sentence_prob_med > all_sentence_prob_graph:\n",
    "            y_hat = 1\n",
    "        else:\n",
    "            y_hat = 0\n",
    "\n",
    "        if y_hat == row['target']:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "    print(\"Accuracy on train set: \",correct/total )\n",
    "    \n",
    "    \n",
    "    ### Do prediction on validation sets using naive bayes algorithm\n",
    "    X_df = pd.DataFrame(Xval)\n",
    "    y_df = pd.DataFrame(Yval)\n",
    "    full_dataset_validate = X_df.copy()\n",
    "    full_dataset_validate['target'] = y_df\n",
    "    \n",
    "    correct = 0\n",
    "    total= 0\n",
    "    for index, row in full_dataset_validate.iterrows():\n",
    "\n",
    "        rs = np.array(row)\n",
    "        indexs = list(np.where(rs>=1)[0])\n",
    "\n",
    "        all_sentence_prob_med = sum([each_word_prob_med[i] for i in indexs if i!=X_df.shape[1]]) * med_prob\n",
    "        all_sentence_prob_graph = sum([each_word_prob_graphics[i] for i in indexs if i!=X_df.shape[1]]) * graphic_prob\n",
    "        if all_sentence_prob_med > all_sentence_prob_graph:\n",
    "            y_hat = 1\n",
    "        else:\n",
    "            y_hat = 0\n",
    "\n",
    "        if y_hat == row['target']:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "    print(\"Accuracy on validation set: \",correct/total )\n",
    "    \n",
    "    ### Do prediction on test sets using naive bayes algorithm\n",
    "    X_df = pd.DataFrame(Xtest)\n",
    "    y_df = pd.DataFrame(Ytest)\n",
    "    full_dataset_test = X_df.copy()\n",
    "    full_dataset_test['target'] = y_df\n",
    "    \n",
    "    correct = 0\n",
    "    total= 0\n",
    "    for index, row in full_dataset_test.iterrows():\n",
    "\n",
    "        rs = np.array(row)\n",
    "        indexs = list(np.where(rs>=1)[0])\n",
    "\n",
    "        all_sentence_prob_med = sum([each_word_prob_med[i] for i in indexs if i!=X_df.shape[1]]) * med_prob\n",
    "        all_sentence_prob_graph = sum([each_word_prob_graphics[i] for i in indexs if i!=X_df.shape[1]]) * graphic_prob\n",
    "        if all_sentence_prob_med > all_sentence_prob_graph:\n",
    "            y_hat = 1\n",
    "        else:\n",
    "            y_hat = 0\n",
    "\n",
    "        if y_hat == row['target']:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "    print(\"Accuracy on test set: \",correct/total )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - 1) Implementing Naive Bayes Classifier for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using bag of words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset to bag of words representation and split the dataset into train, validate and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yaohv_QBFgr",
    "outputId": "e1d415c4-5841-48ff-91d0-58f8ca0c0392"
   },
   "outputs": [],
   "source": [
    "### Here I have used bag of word representation for dataset\n",
    "\n",
    "df = twenty_newsgroup_to_df()\n",
    "corporus, most_freq = make_vocab_with_features(df,10000) ###doing preprocessing on dataset and making dictionary of unique words\n",
    "sentence_vectors = convert_to_bag_of_words(corporus,most_freq) ### calling bag of words function\n",
    "\n",
    "X = sentence_vectors\n",
    "y = np.array(df['target'])\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dw3oCbhFeZEy"
   },
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run main function to get accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yPN1ugdbNfb",
    "outputId": "aa2ac093-c75c-42a1-d673-adb514b34fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set:  0.89\n",
      "Accuracy on validation set:  0.88\n",
      "Accuracy on test set:  0.8897338403041825\n"
     ]
    }
   ],
   "source": [
    "main(Xtrain, Ytrain, Xval, Yval, Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tf-idf word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyUkJrDftZNr"
   },
   "source": [
    "### Convert dataset to TF-IDF word representation and split the dataset into train, validate and test set\n",
    "#### Previously, in case of bag of word, I made 10,000 features for each row. But, here I have just used 1,000 features for each row because it was taking alot of time to run the algorithm when I was using 10,000 features. This is why , I got less accuracy than bag of word represtation. Accuracy can be increased if we use more features for each row!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TcyilUVYsxnP"
   },
   "outputs": [],
   "source": [
    "### Here I have used TF-IDF word representation for dataset\n",
    "\n",
    "df__td_idf = twenty_newsgroup_to_df()\n",
    "df__td_idf['text'].replace('', np.nan, inplace=True)\n",
    "df__td_idf.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "### make_vocab_with_features function second argument describe the number of features to make.\n",
    "corporus_td_idf, most_freq_td_idf = make_vocab_with_features(df__td_idf, 1000)###doing preprocessing on dataset and making dictionary of unique words\n",
    "sentence_vectors_td_idf = convert_to_tf_idf(corporus_td_idf,most_freq_td_idf) ### calling TF-IDF function\n",
    "\n",
    "X = sentence_vectors_td_idf\n",
    "y = np.array(df__td_idf['target'])\n",
    "y = y.astype('int')\n",
    "\n",
    "Xtrain_td_idf, Ytrain_td_idf, Xval_td_idf, Yval_td_idf, Xtest_td_idf, Ytest_td_idf = split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we use above splitted data which is TF-IDF representation of words in main function on respective set and get accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-lBpLj7uV-s",
    "outputId": "305ffc6f-25bc-422e-8bab-25c62a8e1d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set:  0.49642857142857144\n",
      "Accuracy on validation set:  0.47333333333333333\n",
      "Accuracy on test set:  0.547085201793722\n"
     ]
    }
   ],
   "source": [
    "main(Xtrain_td_idf, Ytrain_td_idf, Xval_td_idf, Yval_td_idf, Xtest_td_idf, Ytest_td_idf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m1_qHwi48hn"
   },
   "source": [
    "# Exercise - 2) Implementing SVM Classifier via Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hiJybgON7NM-"
   },
   "outputs": [],
   "source": [
    "### Hyper-paramters space\n",
    "param_svm = [\n",
    "    {'C': [1e-4, 1e-3, 1e-2, 1e-1], 'kernel': ['linear']},\n",
    "    {'C': [120, 200, 500, 1000], 'kernel': ['linear']},\n",
    "    {'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "    {'C': [120, 200, 500, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "### Doing grid search with k-fold cross validation to tune hyper paramters\n",
    "gridsSVC = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid=param_svm,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(n_splits=5),# what type of cross validation to use\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I have used bag of word representation for dataset\n",
    "\n",
    "df = twenty_newsgroup_to_df()\n",
    "corporus, most_freq = make_vocab_with_features(df,1000) ###doing preprocessing on dataset and making dictionary of unique words\n",
    "sentence_vectors = convert_to_bag_of_words(corporus,most_freq) ### calling bag of words function\n",
    "\n",
    "X = sentence_vectors\n",
    "y = np.array(df['target'])\n",
    "y = y.astype('int')\n",
    "Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1],\n",
       "                          'kernel': ['linear']},\n",
       "                         {'C': [120, 200, 500, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [0.0001, 0.001, 0.01, 0.1, 0.5],\n",
       "                          'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
       "                         {'C': [120, 200, 500, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsSVC.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters are:  {'C': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters are: \", gridsSVC.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV automatically uses the optimal hyperparamters for prediction  if we set refit flag to true while optimization. I already set the refit flag to true above so need to first retrain with with optimal hyperparamters while doing prediction on validate and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on respective sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedY_validation = gridsSVC.predict(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy is:  0.89\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Validation Accuracy is: \", accuracy_score(Yval, predictedY_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedY = gridsSVC.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test Accuracy is: 0.8517110266159695\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Test Accuracy is:\", accuracy_score(Ytest, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will use TF-IDF representation of dataset and do hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-paramters space\n",
    "param_svm = [\n",
    "    {'C': [1e-4, 1e-3, 1e-2, 1e-1], 'kernel': ['linear']},\n",
    "    {'C': [120, 200, 500, 1000], 'kernel': ['linear']},\n",
    "    {'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "    {'C': [120, 200, 500, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "### Doing grid search with k-fold cross validation to tune hyper paramters\n",
    "gridsSVC_ = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid=param_svm,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing?\n",
    "    cv=StratifiedKFold(n_splits=5),# what type of cross validation to use\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I have used TF-IDF word representation for dataset\n",
    "\n",
    "df__td_idf = twenty_newsgroup_to_df()\n",
    "df__td_idf['text'].replace('', np.nan, inplace=True)\n",
    "df__td_idf.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "### make_vocab_with_features function second argument describe the number of features to make.\n",
    "corporus_td_idf, most_freq_td_idf = make_vocab_with_features(df__td_idf, 1000)###doing preprocessing on dataset and making dictionary of unique words\n",
    "sentence_vectors_td_idf = convert_to_tf_idf(corporus_td_idf,most_freq_td_idf) ### calling TF-IDF function\n",
    "\n",
    "X = sentence_vectors_td_idf\n",
    "y = np.array(df__td_idf['target'])\n",
    "y = y.astype('int')\n",
    "\n",
    "Xtrain_td_idf, Ytrain_td_idf, Xval_td_idf, Yval_td_idf, Xtest_td_idf, Ytest_td_idf = split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [0.0001, 0.001, 0.01, 0.1],\n",
       "                          'kernel': ['linear']},\n",
       "                         {'C': [120, 200, 500, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [0.0001, 0.001, 0.01, 0.1, 0.5],\n",
       "                          'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
       "                         {'C': [120, 200, 500, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsSVC_.fit(Xtrain_td_idf, Ytrain_td_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters are:  {'C': 120, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters are: \", gridsSVC_.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV automatically uses the optimal hyperparamters for prediction  if we set refit flag to true while optimization. I already set the refit flag to true above so need to first retrain with with optimal hyperparamters while doing prediction on validate and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on respective sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedY_idf_validation = gridsSVC_.predict(Xval_td_idf) ### prediction on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy is:  0.93\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Validation Accuracy is: \",accuracy_score(Yval_td_idf, predictedY_idf_validation) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_idf_Y = gridsSVC_.predict(Xtest_td_idf) ### prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test Accuracy is:  0.8834080717488789\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Test Accuracy is: \", accuracy_score(Ytest_td_idf, predicted_idf_Y))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
